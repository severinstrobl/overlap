name: Trigger and compare benchmarks
on:
  workflow_call:
  workflow_dispatch:

env:
  DEFAULT_REFERENCE_VERSION: develop

jobs:
  benchmark:
    name: Benchmark current version
    uses: ./.github/workflows/run_benchmarks.yaml
    with:
      ref: ${{ github.sha }}

  check_reference:
    name: Check for reference benchmark results
    runs-on: ubuntu-latest
    outputs:
      reference_sha: ${{ steps.obtain_ref.outputs.reference_sha }}
      reference_results_available: ${{ steps.check_ref_results.outputs.available }}
      artifact_id: ${{ steps.check_ref_results.outputs.artifact_id }}
    steps:
      - name: Obtain SHA of reference version
        id: obtain_ref
        if: ${{ github.event_name == 'pull_request' }}
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            reference_sha="${{ github.event.pull_request.base.sha }}"
          else
            reference_sha=$(git rev-parse origin/$DEFAULT_REFERENCE_VERSION)
          fi

          echo "reference_sha=${reference_sha}" >> $GITHUB_OUTPUT

      - name: Check if benchmark results for reference version exist
        id:  check_ref_results
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const ref = "${{ steps.obtain_ref.outputs.reference_sha }}";
            const artifact_name = "benchmarks-" + ref;
            const request = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: artifact_name,
            });

            if (request.data.total_count) {
              console.log(`Benchmark results for revision '${ref}' exist.`);
              core.setOutput("artifact_id", request.data.artifacts[0].id)
            } else {
              console.log(`Benchmark results for revision '${ref}' do not exist, requesting run.`);
            }

            core.setOutput("available", request.data.total_count != 0)

  benchmark_reference:
    name: Benchmark reference version
    needs: check_reference
    if: ${{ !fromJSON(needs.check_reference.outputs.reference_results_available) }}
    uses: ./.github/workflows/run_benchmarks.yaml
    with:
      ref: ${{ needs.check_reference.outputs.reference_sha }}
      allow_failure: true

  compare_benchmarks:
    name: Compare benchmark results
    needs: [benchmark, benchmark_reference, check_reference]
    if: ${{ always() && (fromJSON(needs.check_reference.outputs.reference_results_available) || needs.benchmark_reference.outputs.outcome == 'success') }}
    runs-on: ubuntu-latest
    steps:
      - name: Dependencies
        run: |
          python -m pip install pyperf

      - name: Obtain archived benchmark results for reference version
        if: ${{ fromJSON(needs.check_reference.outputs.reference_results_available) }}
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require("fs");
            const child_process = require("child_process");

            const ref = "${{ needs.check_reference.outputs.reference_sha }}";
            const artifact_id = ${{ needs.check_reference.outputs.artifact_id }};
            const artifact_name = `benchmarks-${ref}`;
            const request = await github.rest.actions.downloadArtifact ({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifact_id,
              archive_format: "zip",
            });

            fs.writeFileSync(artifact_name, Buffer.from(request.data));
            child_process.execSync(`unzip -o ${artifact_name} -d ${ref}`);

      - name: Obtain new benchmark results for reference version
        if: ${{ !fromJSON(needs.check_reference.outputs.reference_results_available) }}
        uses: actions/download-artifact@v3
        with:
          name: benchmarks-${{ needs.check_reference.outputs.reference_sha }}
          path: ${{ needs.check_reference.outputs.reference_sha }}

      - name: Obtain benchmark results for current version
        uses: actions/download-artifact@v3
        with:
          name: benchmarks-${{ github.sha }}
          path: ${{ github.sha }}

      - name: Compare benchmark results
        run: |
          mv ${{ github.sha }}/benchmarks.json Current
          mv ${{ needs.check_reference.outputs.reference_sha }}/benchmarks.json Reference
          python -m pyperf compare_to --table --table-format md Reference Current > report.md

      - name: Create PR comment
        if: ${{ github.event_name == 'pull_request' }}
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require("fs");
            const report = fs.readFileSync("./report.md")
              .toString()
              .replace(/Current/, "Current (${{ github.sha }})")
              .replace(/Reference/, "Reference (${{ needs.check_reference.outputs.reference_sha }})");

            const msg = `## C++ benchmark results\n${report}`;
            try {
              const result = await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: msg,
              })
              return result.data;
            } catch (err) {
              core.setFailed(`Request failed with error ${err}`);
            }

      - name: Report via annotation
        if: ${{ github.event_name != 'pull_request' }}
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            core.notice('## C++ benchmark results\n# + fs.readFileSync('./report.md'));
