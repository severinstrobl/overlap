name: Run benchmarks
on:
  workflow_call:
    inputs:
      ref:
        description: "Branch, tag, or SHA1 hash to benchmark"
        required: true
        type: string
      allow_failure:
        description: "Allow workflow to continue on failure"
        required: false
        default: false
        type: boolean
    outputs:
      outcome:
        description: "Outcome of the actual benchmarking job"
        value: ${{ jobs.benchmark_cpp.outputs.outcome }}

jobs:
  benchmark_cpp:
    name: C++ benchmarks
    runs-on: ubuntu-24.04
    outputs:
      outcome: ${{ steps.benchmark_step.outcome }}
    steps:
      - name: Checkout
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          ref: ${{ inputs.ref }}
          submodules: true

      - name: Dependencies
        run: |
          python -m pip install pyperf

      - name: Build and run benchmark for selected revision
        id: benchmark_step
        continue-on-error: ${{ inputs.allow_failure }}
        run: |
          cmake \
            -Bbuild \
            -DCMAKE_BUILD_TYPE=Release \
            -DOVERLAP_WITH_BENCHMARKS=ON \
            -DOVERLAP_WITH_TESTS=OFF

          cmake --build build

          ctest --test-dir build -L benchmark

          mapfile -t results < <(shopt -s globstar; ls build/benchmarks/**/*.json)
          cp "${results[0]}" benchmarks-cpp.json

          for f in "${results[@]:1}"; do
            python -m pyperf convert --stdout --add="$f" benchmarks-cpp.json | jq > combined.json
            mv combined.json benchmarks-cpp.json
          done

          echo "ref_sha=$(git rev-parse ${{ inputs.ref }})" >> "$GITHUB_OUTPUT"

      - name: Store benchmark results for selected revision
        if: ${{ steps.benchmark_step.outcome == 'success' }}
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: benchmarks-cpp-${{ steps.benchmark_step.outputs.ref_sha }}
          path: benchmarks-cpp.json
  benchmark_python:
    name: Python benchmarks
    runs-on: ubuntu-24.04
    outputs:
      outcome: ${{ steps.benchmark_step.outcome }}
    steps:
      - name: Checkout
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          ref: ${{ inputs.ref }}
          submodules: true

      - name: Dependencies
        run: |
          python -m pip install pyperf

      - name: Build and run benchmark for selected revision
        id: benchmark_step
        continue-on-error: ${{ inputs.allow_failure }}
        run: |
          python -m pip install .
          python benchmarks/python/run_benchmarks.py
          cp benchmarks/python/overlap_python.json benchmarks-python.json

          echo "ref_sha=$(git rev-parse ${{ inputs.ref }})" >> "$GITHUB_OUTPUT"

      - name: Store benchmark results for selected revision
        if: ${{ steps.benchmark_step.outcome == 'success' }}
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: benchmarks-python-${{ steps.benchmark_step.outputs.ref_sha }}
          path: benchmarks-python.json
